{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os.path as op\n",
    "import yaml\n",
    "from IPython.display import Markdown, Latex\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "\n",
    "with open('../codecheck.yml') as f:\n",
    "    codecheck_conf = yaml.safe_load(f)\n",
    "\n",
    "def name_orcid(entry):\n",
    "    'Helper function for Name + ORCID'\n",
    "    return f\"{entry['name']} (ORCID: [{entry['ORCID']}](https://orcid.org/{entry['ORCID']}))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5893809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in basic info from manifest file:\n",
    "Markdown(f'''# CODECHECK certificate {codecheck_conf['certificate']}{{-}}\n",
    "## [{codecheck_conf['report'].split('://')[1]}]({codecheck_conf['report']}) {{-}}\n",
    "[![CODECHECK logo](codecheck_logo.png)](https://codecheck.org.uk)''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0dbd3",
   "metadata": {},
   "source": [
    "## CODECHECK summary{-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_header = '''\n",
    "Item | Value\n",
    ":--- | :----\n",
    "'''\n",
    "summary_rows = [\n",
    "    f\"Title | *{codecheck_conf['paper']['title']}*\",\n",
    "    f\"Authors | {', '.join([name_orcid(a) for a in codecheck_conf['paper']['authors']])}\",\n",
    "    f\"Reference | [{codecheck_conf['paper']['reference'].split('://')[1]}]({codecheck_conf['paper']['reference']})\",\n",
    "    f\"Repository | [{codecheck_conf['repository'].split('://')[1]}]({codecheck_conf['repository']})\",\n",
    "    f\"Codechecker | {name_orcid(codecheck_conf['codechecker'])}\",\n",
    "    f\"Date of check | {datetime.fromisoformat(codecheck_conf['check_time']).date()}\",\n",
    "    f\"Summary | {codecheck_conf['summary'].strip()}\",\n",
    "]\n",
    "Markdown(summary_header + '\\n'.join(summary_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a52c9",
   "metadata": {},
   "source": [
    "## Summary of output files generated{-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save some space, we remove the directory name\n",
    "files_header = '''\n",
    "File | Comment | Size\n",
    ":----| :------ | ---:\n",
    "'''\n",
    "files_rows = [\n",
    "    ('`' + op.basename(entry['file']) + '` | ' +\n",
    "        entry.get('comment', '') + ' | ' +\n",
    "        str(op.getsize(op.join('outputs', entry['file']))))\n",
    "    for entry in codecheck_conf['manifest']\n",
    "]\n",
    "Markdown(files_header + '\\n'.join(files_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b94d5f",
   "metadata": {},
   "source": [
    "## Summary{-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d319d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(codecheck_conf['summary'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63e1d6",
   "metadata": {},
   "source": [
    "## CODECHECKER notes{-}\n",
    "\n",
    "### Workflow{-}\n",
    "The original repository for the code was located at [github.com/tedinburgh/causality-review](https://github.com/tedinburgh/causality-review), and an earlier version had been archived at [zenodo.org/record/4657015](https://zenodo.org/record/4657015). \n",
    "I forked the repository at commit `010aa51a80d91857bea4f0aa33885183022ce59d` to [github.com/codecheckers/causality-review](https://github.com/codecheckers/causality-review) and started the CODECHECK. The original repository already contained a `codecheck.yml` MANIFEST, as well as a `README.md` file detailing the steps to run the code, a `requirements.txt` file stating the dependencies (with minimal versions), and a `codecheck-instructions.sh` script to automatically execute the steps detailed in the README file. The script can be downloaded individually; executing it will download the GitHub repository, set up a conda environment and run all the steps to reproduce the results. Since I already had cloned the full repository, I did not execute the script but instead only run the steps following the cloning of the repository. As suggested by the authors, I only reproduced one part of the simulation results (linear processes), since re-running all the simulations would have taken too long. All other figures and data tables were regenerated from stored results also present in the repository (`simulation-data/`).\n",
    "\n",
    "### Execution of the workflow{-}\n",
    "I ran everything on a somewhat outdated workstation (Intel(R) Xeon(R) CPU E5-1630 v3 @ 3.70GHz, 16GB RAM) on Ubuntu Linux 18.04. The simulation time was 4 hours, comparable to the 3 hours stated by the authors. Regenerating the figures took only about 1 minute, significantly shorter than the \"up to 15 minutes\" suggested by the authors. Creating the figures emitted a number of warnings (see below), but none of them seemed to affect the output and all figures were created successfully.\n",
    "\n",
    "#### Output from running `python causality-review-code/misc_ci.py`{-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat outputs/figures_err.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06edf3",
   "metadata": {},
   "source": [
    "### Comparison of results with author repository{-}\n",
    "By visual inspection, all regenerated figures are identical to the figures present in the repository. Comparison with `git diff-image` ([github.com/ewanmellor/git-diff-image](https://github.com/ewanmellor/git-diff-image)) showed minimal differences in some regions of the color plots of `hb_figure1.{pdf,eps}` and `hb_figure2.{pdf,eps}`, but these differences were not discernible by naked eye and seem to reflect very minor numerical differences. Given that my figures were generated with matplotlib 3.3.4 (see package versions at the end of this document) and the authors generated figures with 3.3.2, I suspected this version difference to be the reason, but a cursory check with a downgraded matplotlib did not change the result.\n",
    "The generated file `ul-transforms.txt` (underlying Table III in the paper) is identical to the file in the repository, except for some irrelevant differences between `0.000` and `-0.000`.\n",
    "\n",
    "The simulation results for the linear process simulations stored in `lp_values.csv` differ slightly in columns 9–12 and 19–20, reflecting very minor numerical differences. After rounding all values to 10 decimal digits, the results were exactly identical.\n",
    "\n",
    "Since the file `lp_times.csv` contains execution times measured during the run of this CODECHECK, it differs from the files provided by the authors. This also holds for the column representing the values for the linear processes simulations in file `computational-times.txt`. The results do seem comparable to the authors' results, though, and the order of the methods is preserved. See below for a graphical comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lp_times(fname):\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    # extract first two columns\n",
    "    methods, means, stds = [], [], []\n",
    "    for line in lines:\n",
    "        method, run_times = [l.strip() for l in line.split('&')[:2]]\n",
    "        mean_time, std_time = run_times[:5], run_times[7:12]\n",
    "        methods.append(method)\n",
    "        means.append(float(mean_time))\n",
    "        stds.append(float(std_time))\n",
    "    return methods, means, stds\n",
    "orig_methods, orig_means, orig_stds = extract_lp_times(op.join('..', 'figures', 'computational-times.txt'))\n",
    "repr_methods, repr_means, repr_stds = extract_lp_times(op.join('outputs', 'figures', 'computational-times.txt'))\n",
    "assert orig_methods == repr_methods\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.set_yscale('log')\n",
    "ax.errorbar(orig_methods, orig_means, orig_stds, fmt='o', label='original')\n",
    "ax.errorbar(repr_methods, repr_means, repr_stds, fmt='o', label='reproduction')\n",
    "ax.set_title('Computational requirements of linear process simulations (cf. first column of Table S.II in paper)')\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274527dc",
   "metadata": {},
   "source": [
    "### Comparison of results with arXiv preprint{-}\n",
    "#### Table III{-}\n",
    "The `codecheck.yml` manifest notes that the arXiv preprint has a small error in Table III in the baseline column for methods \"TE (H)\" and \"ETE (H)\", which I can confirm: the results in the repository state $\\langle \\mu \\rangle = 0.675$ (\"TE (H)\") and $\\langle \\mu \\rangle = 0.674$ (\"ETE (H)\"), wheras the paper states $\\langle \\mu \\rangle = 0.673$ for both. However, I identified additional differences in the Gaussian noise column for the \"NLGC\" and \"CCM\" methods:\n",
    "\n",
    "**Paper**\n",
    "\n",
    "Method | $\\sigma^2_G$ = 0.1 | $\\sigma^2_G$ = 1 | $\\sigma^2_G$ = 1\n",
    ":------|-------------------:|-----------------:|----------------:\n",
    "NLGC   |0.030               | 0.741            | -0.003\n",
    "&nbsp; |0.972               | 1.335            | 2.313\n",
    "CCM    |0.005               | 0.176            | -0.136\n",
    "&nbsp; |0.981               | 0.986            | 0.951\n",
    "\n",
    "**Repository (file `ul-transforms.txt`):**\n",
    "\n",
    "Method | $\\sigma^2_G$ = 0.1 | $\\sigma^2_G$ = 1 | $\\sigma^2_G$ = 1\n",
    ":------|-------------------:|-----------------:|----------------:\n",
    "NLGC   |0.031               | 0.740            | -0.007\n",
    "&nbsp; |1.023               | 1.345            | 2.325\n",
    "CCM    |0.013               | 0.151            | -0.075\n",
    "&nbsp; |1.010               | 0.944            | 0.959\n",
    "       \n",
    "#### Figure 4{-}\n",
    "The top part of Figure 4 (file `ul_figure.{pdf,eps}`) uses a different y axis scale for the EGC method in the repository file compared to the one included in the paper. As far as I can tell, the plotted values appear to be the same, i.e. it is just a question of \"zoom level\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f632fdd",
   "metadata": {},
   "source": [
    "#### Figure S1{-}\n",
    "\n",
    "There appears to be a small difference between the Figure S1 used in the arXiv preprint and the one in the repository (file `corr_transforms_plots.{pdf,eps}`). To confirm, I used the `pdfimages` tool to extract a png of the color plot from both the paper PDF and from the repository version, and plot them side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_version = mpimg.imread('outputs/extracted_S1_paper.png')\n",
    "repo_version = mpimg.imread('outputs/extracted_S1_repo.png')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 6))\n",
    "ax1.imshow(paper_version)\n",
    "ax1.axis('off')\n",
    "ax2.imshow(repo_version)\n",
    "ax2.axis('off')\n",
    "ax1.set_title('paper version')\n",
    "_ = ax2.set_title('repository version')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0bdb9",
   "metadata": {},
   "source": [
    "While the difference is small, it seems to be too big to be simply explained by e.g. a color conversion process (note the differences in the lower left corner)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e0395a",
   "metadata": {},
   "source": [
    "## Recommendation to the authors{-}\n",
    "Overall, the authors provide very thorough and easy-to-follow steps for reproduction, and make it conveniently possible to only reproduce parts of their study by calling the respective scripts with command line arguments. Apart from clearing up the minor discrepancies detailed in the report above, I only have a few minor recommendations:\n",
    "\n",
    "* It would be preferrable to have only one file for each figure instead of one PDF and one EPS version. Automatic treatement of the manifest file is also slightly impaired by the fact that the file comment is formally only attached to the EPS file entry but refers to both files.\n",
    "* It would be helpful to clearly state if files are not expected to be reproduced exactly, e.g. if they represent measured execution times instead of calculated values (`lp_times.csv`, `computation-times.txt`).\n",
    "* Long simulation runs (in this CODECHECK, the linear process simulations) would benefit from some indication of how much time (or how many iterations) is still needed to complete the run.\n",
    "* The bold formatting in the tables (indicating e.g. minimum values per column) seems to have been added manually after the automatic generation of the tables. To avoid errors, it might make sense to have the code also take care of this highlighting.\n",
    "* A very minor point: the `codechecker-instructions.sh` script contained in the repository is meant to be independent of the repository and starts by cloning it. It is unclear in what situation someone would have access to this script file but not have already cloned the repository. It might have been more straightforward to state in the README file to clone the repository, and then ask the user to execute the script file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4327aa",
   "metadata": {},
   "source": [
    "## Citing this document{-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f\"{codecheck_conf['codechecker']['name']} \"\n",
    "         f\"({datetime.fromisoformat(codecheck_conf['check_time']).year}). \"\n",
    "         f\"CODECHECK Certificate {codecheck_conf['certificate']}. \"\n",
    "         f\"Zenodo. [{codecheck_conf['report'].split('://')[1]}]({codecheck_conf['report']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f5d71",
   "metadata": {},
   "source": [
    "## About CODECHECK{-}\n",
    "This certificate confirms that the codechecker could independently reproduce the results of a computational analysis given the data and code from a third party. A CODECHECK does not check whether the original computation analysis is correct. However, as all materials required for the reproduction are freely availableby following the links in this document, the reader can then study for themselves the code and data.\n",
    "\n",
    "## About this document{-}\n",
    "This document was created using a [jupyter notebook](https://jupyter.org/) and converted into PDF via [nbconvert](https://nbconvert.readthedocs.io/), [pandoc](https://pandoc.org/), and [xelatex](http://xetex.sourceforge.net/). The command `make codecheck.pdf` will regenerate the report file.\n",
    "\n",
    "## License{-}\n",
    "The code, data, and figures created by the original authors are licensed under the MIT license (see their [LICENSE file](https://github.com/codecheckers/causality-review/blob/main/LICENSE)). The content of the `codecheck` directory and this report are licensed under the [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9051d",
   "metadata": {},
   "source": [
    "## Package versions{-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat outputs/conda_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b01948",
   "metadata": {},
   "source": [
    "## Manifest files{-}\n",
    "\n",
    "### CSV files{-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86962465",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_markdown = []\n",
    "for entry in codecheck_conf['manifest']:\n",
    "    fname = entry['file']\n",
    "    if not fname.endswith('.csv'):\n",
    "        continue\n",
    "    comment = entry.get('comment', None)\n",
    "    df = pd.read_csv(op.join('outputs', fname), index_col=False, header=None)\n",
    "    markdown = f'''### `{fname}` {{-}}\n",
    "{('Author comment: *' + comment + '*') if comment else ' '}\n",
    "\n",
    "**Column summary statistics:**\n",
    "\n",
    "{df.describe().transpose().to_markdown(tablefmt=\"grid\",\n",
    "                                       floatfmt=('.0f', '.0f', '.4f', '.4f', '.4f', '.4f', '.4f', '.4f', '.4f'))}\n",
    "'''\n",
    "    full_markdown.append(markdown)\n",
    "\n",
    "Markdown('\\n\\n'.join(full_markdown))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06133d4",
   "metadata": {},
   "source": [
    "### LaTeX tables{-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008c859",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "outputs": [],
   "source": [
    "# LaTeX tables (only correctly displayed in LaTeX output/PDF)\n",
    "# Hardcoded names for columns\n",
    "columns = {'figures/computational-times.txt': '{lrrrrrrrr}',\n",
    "           'figures/ul-transforms.txt': '{lllrrrrrrrrrrrr}'}\n",
    "full_text = []\n",
    "for entry in codecheck_conf['manifest']:\n",
    "    fname = entry['file']\n",
    "    if not fname.endswith('.txt'):\n",
    "        continue\n",
    "    assert fname in columns\n",
    "    header = [r'\\texttt{' + fname.replace('_', r'\\_') + r'}\\\\',\n",
    "              r'Author comment: \\emph{' + entry.get('comment', '') + r'}\\\\', '',\n",
    "              r'\\begin{tiny}\\begin{tabular}' + columns[fname]]\n",
    "    footer = [r'\\end{tabular}\\end{tiny}', '', '']\n",
    "    full_text.extend(header + [r'\\input{outputs/' + fname + r'}'] + footer)\n",
    "Latex('\\n'.join(full_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1424a39",
   "metadata": {},
   "source": [
    "### Figures{-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1bccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaTeX tables (only correctly displayed in LaTeX output/PDF)\n",
    "# The comments are only stored for the eps versions, copy them over\n",
    "eps_comments = {entry['file']: entry.get('comment', '')\n",
    "                for entry in codecheck_conf['manifest']\n",
    "                if entry['file'].endswith('.eps')}\n",
    "full_text = []\n",
    "# Figures (only PDF versions)\n",
    "for entry in codecheck_conf['manifest']:\n",
    "    fname = entry['file']\n",
    "    if not fname.endswith('.pdf'):\n",
    "        continue\n",
    "    comment = eps_comments[fname[:-4] + '.eps']\n",
    "    full_text.extend([r'\\texttt{' + fname.replace('_', r'\\_') + r'}.\\\\',\n",
    "                      r'Author comment: \\emph{' + comment + r'}\\\\',\n",
    "                      r'\\includegraphics{outputs/' + fname + r'}',\n",
    "                      '', ''])\n",
    "Latex('\\n'.join(full_text))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
